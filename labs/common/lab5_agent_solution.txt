#!/usr/bin/env python3
"""
Lab 5: RAG-Enhanced Agentic Weather Agent
────────────────────────────────────────────────────────────────────
A true agentic RAG workflow combining:
- Lab 2's agent pattern (TAO loop with LLM-driven tool selection)
- Lab 3's MCP server (weather, geocoding, and now RAG search tools)
- Lab 4's vector database (ChromaDB with office PDF data)

The LLM controls the entire workflow, deciding which tools to call
and when to stop — just like the agents in Labs 2 and 3.

Tools Available to the Agent (ALL via MCP server)
--------------------------------------------------
1. search_offices(query) → text chunks from office vector DB
2. geocode_location(name) → lat/lon coordinates
3. get_weather(lat, lon)  → current weather in Celsius
4. convert_c_to_f(c)      → temperature in Fahrenheit

Example Agent Flow
------------------
  User: "Tell me about HQ"
  → Agent calls search_offices("HQ") → gets office info with city
  → Agent calls geocode_location("Austin") → gets coordinates
  → Agent calls get_weather(30.27, -97.74) → gets weather data
  → Agent calls convert_c_to_f(25.0) → gets Fahrenheit
  → Agent says DONE → displays collected results

Prerequisites
-------------
- MCP server running: python mcp_server.py (Lab 5 version with search_offices)
"""

# ────────────────────────── standard libs ───────────────────────────
import asyncio
import json
import re
import textwrap
from pathlib import Path

# ────────────────────────── third-party libs ────────────────────────
from fastmcp import Client
from fastmcp.exceptions import ToolError
from langchain_ollama import ChatOllama

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 1.  Configuration                                               ║
# ╚══════════════════════════════════════════════════════════════════╝
MCP_ENDPOINT     = "http://127.0.0.1:8000/mcp/" # MCP server from Lab 5

# Regex for parsing LLM responses (same pattern as Labs 2 and 3)
ACTION_RE = re.compile(r"Action:\s*(\w+)", re.IGNORECASE)
ARGS_RE   = re.compile(r"Args:\s*(\{.*?\})(?:\s|$)", re.S | re.IGNORECASE)

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 2.  MCP result unwrapper                                        ║
# ╚══════════════════════════════════════════════════════════════════╝
def unwrap(obj):
    """Extract plain Python values from FastMCP result wrappers."""
    if hasattr(obj, "structured_content") and obj.structured_content:
        return unwrap(obj.structured_content)
    if hasattr(obj, "data") and obj.data:
        return unwrap(obj.data)
    if hasattr(obj, "text"):
        try:
            return json.loads(obj.text)
        except Exception:
            return obj.text
    if hasattr(obj, "value"):
        return obj.value
    if isinstance(obj, list) and len(obj) == 1:
        return unwrap(obj[0])
    if isinstance(obj, dict):
        if len(obj) == 1:
            return unwrap(list(obj.values())[0])
        numeric_vals = [v for v in obj.values() if isinstance(v, (int, float))]
        if len(numeric_vals) == 1:
            return numeric_vals[0]
    return obj

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 3.  System prompt — tells the LLM about all available tools     ║
# ╚══════════════════════════════════════════════════════════════════╝
SYSTEM = textwrap.dedent("""
You are an office information agent. You answer questions about company
offices by searching a database and looking up live weather data.

You have these tools:

search_offices(query: str)
    Searches the company office database for matching information.
    Returns: text chunks with office names, cities, and details.
    ALWAYS call this first to find relevant office information.

geocode_location(name: str)
    Converts a city/location name to coordinates.
    Returns: {"latitude": float, "longitude": float, "name": str}

get_weather(lat: float, lon: float)
    Gets current weather for given coordinates.
    Returns: {"temperature": float, "code": int, "conditions": str}
    Note: temperature is in Celsius.

convert_c_to_f(c: float)
    Converts a Celsius temperature to Fahrenheit.
    Returns: float

For each step, respond with EXACTLY these three lines:

Thought: <your reasoning about what to do next>
Action: <exact tool name only>
Args: <valid JSON arguments for the tool>

Examples:

Thought: I need to search for office information about HQ
Action: search_offices
Args: {"query": "HQ"}

Thought: I found the office is in Austin, TX. I need coordinates.
Action: geocode_location
Args: {"name": "Austin"}

Thought: Now I'll get the weather at those coordinates
Action: get_weather
Args: {"lat": 30.2672, "lon": -97.7431}

Thought: I need to convert 25.0 Celsius to Fahrenheit
Action: convert_c_to_f
Args: {"c": 25.0}

When you have gathered all the information, respond with:
Thought: I have all the information needed
Action: DONE
Args: {}

Do NOT put arguments on the Action line. Arguments go ONLY in the Args line as JSON.

RULES:
1. ALWAYS start with search_offices to find office data
2. The FIRST search result is the most relevant — use the city from it
3. When geocoding, use ONLY the city name (e.g. "New York" not "New York, NY")
4. If geocoding fails, retry with a simpler name before trying other cities
5. Get the weather, then convert the temperature
6. Do NOT make up data — only use what the tools return
7. Do NOT add extra text beyond the three lines
""").strip()

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 4.  TAO agent loop — the LLM decides which tools to call        ║
# ╚══════════════════════════════════════════════════════════════════╝
async def run(prompt: str, max_steps: int = 10) -> None:
    """
    Run the agentic RAG loop where the LLM drives the workflow.

    ALL tools go through the MCP server — the agent is a pure
    orchestrator that never touches the database or APIs directly.
    """
    llm = ChatOllama(model="llama3.2:latest", temperature=0.0)

    messages = [
        {"role": "system", "content": SYSTEM},
        {"role": "user",   "content": prompt},
    ]

    # Track gathered data for the final display
    context = {
        "office_info": None,
        "city": None,
        "conditions": None,
        "temp_f": None,
    }

    print("\n" + "="*60)
    print("RAG Agent — Thought / Action / Observation")
    print("="*60 + "\n")

    async with Client(MCP_ENDPOINT) as mcp:
        for step in range(1, max_steps + 1):
            print(f"[Step {step}]")

            # Ask the LLM what to do next
            response = llm.invoke(messages).content.strip()
            print(response)

            # Parse the Action from the response
            action_match = ACTION_RE.search(response)
            if not action_match:
                print("\nError: Could not parse Action from response\n")
                break

            action = action_match.group(1).lower()

            # ── Check if the agent decided it is done ─────────────────
            if action == "done":
                print("\n" + "="*60)
                print("Agent completed!\n")
                if context["office_info"]:
                    print(f"Office: {context['office_info']}")
                if context["conditions"] and context["temp_f"] is not None:
                    print(f"Weather: {context['conditions']}, "
                          f"{context['temp_f']:.1f} °F")
                print()
                return

            # ── Parse the Args ────────────────────────────────────────
            args_match = ARGS_RE.search(response)
            if not args_match:
                print("\nError: Could not parse Args from response\n")
                break

            try:
                args = json.loads(args_match.group(1))
            except json.JSONDecodeError as e:
                print(f"\nError: Invalid JSON: {e}\n")
                break

            # ── Call the tool via MCP ─────────────────────────────────
            print(f"\n-> Calling: {action}({json.dumps(args)})")

            try:
                raw = await mcp.call_tool(action, args)
                result = unwrap(raw)
            except ToolError as e:
                result = f"Error: {e}"
            except Exception as e:
                result = f"Error: {type(e).__name__}: {e}"

            # Store relevant context from tool results
            if action == "search_offices":
                context["office_info"] = str(result).split("\n")[0][:200]
            elif action == "geocode_location" and isinstance(result, dict):
                context["city"] = result.get("name")
            elif action == "get_weather" and isinstance(result, dict):
                context["conditions"] = result.get("conditions")
            elif action == "convert_c_to_f" and isinstance(result, (int, float)):
                context["temp_f"] = float(result)

            # Format the observation and show it
            if isinstance(result, (dict, float, int)):
                obs_text = json.dumps(result)
            else:
                obs_text = str(result)
            print(f"Observation: {obs_text}\n")

            # Feed the observation back to the LLM
            messages.append({"role": "assistant", "content": response})
            messages.append({"role": "user",
                             "content": f"Observation: {obs_text}"})

        print(f"\nReached maximum steps ({max_steps}).\n")

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 5.  Interactive loop                                             ║
# ╚══════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    print("="*60)
    print("RAG-Enhanced Office Weather Agent")
    print("="*60)
    print("\nAsk about any office (e.g. 'Tell me about HQ')")
    print("Type 'exit' to quit\n")

    while True:
        prompt = input("User: ").strip()
        if prompt.lower() == "exit":
            print("Goodbye!")
            break
        if prompt:
            asyncio.run(run(prompt))
            print()
